## Lesson 1: Introduction to cyber security psychology and behaviour 
### Lecture 1: What is [[security behaviour]]?
- ðŸš¨ Oxford English Dictionary definition klaxon ðŸš¨
	- Pointless. Stupid. Only Abigail Thorn does this well
- And now Cambridge's definition!
- Proposed model has three dimensions
	- Traits that are inherent
	- Contextual/environmental triggers
	- Situational circumstances
- Different models can be used to analyse human behaviour. For example,
	- Descriptive: defining how and why people behave the way they do
	- Normative: a model based on a theoretical hyper-rational person
		- (cf. neoliberal economics)
	- Prescriptive: what an individual should do (according to whom!?) in a situation, without getting into the why or the how
- So we might define [[security behaviour]] as ![[security behaviour#Definition]]
- So we're now going to take a quick step sideways into economics and philosophy by defining [[utility]]
![[utility#Definition]]
- Through the lens of utility, behaviour is the collected actions of an individual that aim to increase positive utility and decrease negative utility or, more bluntly: increase pleasure and decrease suffering
	- hmm. 
- There is then a hand-wavy bit about how not everyone does always try to maximise utility, because their own conception of utility is different. Then some more hand-waving about how all models are wrong, which I do actually agree with: all models are wrong, but some are useful

### Lecture 2: Behavioural economics and cyber security 1
Oh goodie, behavioural economics!

Behavioural economics uses classical economics and combines it with insights from psychology to identify why an otherwise rational human would deviate from the 'rational' course of action. In other words, it reframes the rationality by taking a more holistic view of the individual.

Apparently there are four 'C's that constitute rationality:
- completeness: that the decision maker has perfect information about all possible courses of actions, and can compare them perfectly
	- This is obviously impossible
- Cognition: the decision maker is free from any cognitive effect that might affect their analysis
	- This is also obviously bullshit
- Computation: the ability to perfectly analyse the information available to them
	- lol
- Consistency: that is, consisteny with 'axioms of choice'
	- oh god what now dot gif
This is just a long run up at pointing out that this is all an incomplete theory for human behaviour, which, yeah.

The economist Herbert Simon coined the term 'boundedly rational'; that is, irrational with the lens of an external observer with perfect knowledge, but rational within their context. Consider also 'ecologically rational', in the sense that we are not independent from our context and circumstances (not sure how different that is to the previous idea). We might also consider 'selective rationality', where people choose to do irrational things that they know are irrational (as opposed to an irrational choice that seems rational to them). We're getting a little meta, but sometimes the rational choice is to do the thing that you know to be irrational, because there is a danger/cost associated with making the rational choice.

We now move on to cognitive biases and heuristics (damn, I should reread [[Thinking Fast and Slow]], because the lecturer very clearly has ðŸ˜†). 

Heuristics are an example of what Daniel Kahneman calls 'System One' thinking: it's quick, and it gives us an answer, but that answer can be wrong. It can also be easily fooled - consider the quick and slow answers to this question

>[!question]
> A baseball bat and a baseball cost $1.10 together. The bat costs $1 more than the ball. How much does the ball cost?

The first is the availability heuristic: we value information that we can recall quickly over information that we need to think about. Then we turn to the anchoring heuristic. That is, information we've received in the context of a question will bias our answer to that question. This is often linked to things like salary negotiations, and is a good reason for you to open those negotiations with a higher number than you expect. If you're lowballed, don't even enter into negotiations, because you'll be subconsciously anchored to that lower value.

Then there's the default heuristic, which could be expressed as Newton's First Law of Motion: an object will continue its current vector (including when that vector is 0) unless acted on by another force. Folks will follow the default path, if it is easier than changing. 

### Lecture 3: Behavioural economics and cyber security 2

In this lecture, we'll cover:
- bounded rationality
- risk, ambiguity, and uncertainty
- loss aversion and framing effects
#### Bounded rationality
- People have limited resources: cognition, time, knowledge, information, memory
- To deal with this, we evolved heuristics: mental shortcuts that are frugal with these resources
- Satisficing: humans tend to seek satisfactory over perfect solutions
- Important to be aware of this heuristic when seeking a solution to the security problem in front of you
#### Risk, ambiguity, uncertainty
>[!question] The price of certainty 
>Suppose there is a lottery whereby the winners are required to sacrifice $1m dollars. A very strange tax system, we imagine. It is drawn by lots, and there is a 1 in 5 chance that you will draw the winning (losing!) ball. However: you know that you can buy your way out of the lottery. The adjudicator is immensely corrupt. 
>
>What would you pay to escape this fate?

We can accurately quantify this [[risk]]. A 20% chance of it happening, with an associated cost of $1m, means you should pay no more than $200,000 to escape the lottery. If you are willing to pay more, reasoning that - say - $400,000 is nothing, compared to the million - you are risk averse, but might well spend $2m over a five year period to avoid a $1m penalty. This is irrational: but so is the reverse. Spending less - risk seeking behaviour - seems irrational too, as you've not done enough to mitigate the risk.

Now, we may not always be able to give such a specific view. The number of balls might vary - as might the fine. However, as long as we have an idea of the range, we can at least take a view on the cost-effectiveness of interventions.

Uncertainty is worse. It may be that rather than balls in the lottery jar, you may reach in and find a spider. 

Most animals, including humans, are increasingly averse as we go from quantified risk, to ambiguity, to uncertainty. We apply heuristics to collapse these spaces into smaller, more manageable spaces - though of course, that only happens in our minds. The problem space is still a complex and uncertain one.

Into this casserole we also throw the unbalanced loss aversion/gain seeking attitude. Generally speaking, we are more sensitive to losing $100 than we are to gaining that same $100. $100 here stands for anything that we own, or might own, or generally value. Money is simply easier, and its value is fungible.

Framing effects are the way that we ask questions or frame ideas. Consider the difference between "20% fat" and "80% lean". Same thing - one hopes - but the mental image we get off them is quite different.

Has your team won 10 games this season - or lost 12? 

Note that the further away the gains/losses, the less sensitive we are to them