---
tags:
- Brookshear, J
---
## Chapter 1: Data storage
### 1.1 Bits and their storage
Covers Boolean logic, gates, truth-tables. Then covers something called a [[flip-flop]], which I LOVE. We then move on to [[hexadecimal]]. The author doesn't actually cover truth-tables properly, which is a shame, because they'd be really helpful when answering the questions at the end of this section
### 1.2 Main memory
As above, so below. Computers, it turns out, have something called a [[cell]]. We touch on RAM, DRAM, and SDRAM, and then move on to how we measure memory capacity. The author has a bit of a whinge that in other scientific fields, _kilo-_ as a prefix means $x \times 10^3$ , whereas in computing it means "$2^{10}$ looks a bit like 1000, if you squint". Ditto the fact that _mega-_ means $x \times 10^6$ , whereas in computers it's $1024^2$ or $2^20$, which is 1,048,576 bits.
### 1.3 [[mass storage]]
The book is a little out of date, claiming that mass storage requires mechanical motion. This was true with the old style of hard drive, but modern [[SDD]] have done away with that need. However, it's worth a a quick review of data storage, because it's always been about converting things to [[bit|bits]], one way or the other
### 1.4 Representing information as bit patterns
### 1.5 The [[binary]] system
Already pretty clear about this, no notes
### 1.6 Storing integers
Couple of different approaches here: [[two's complement]] and [[excess notation]]. The author uses 'merely' and 'simple' in ways I find very snotty: it's not simple.
### 1.7 Storing fractions
Fractions. Now here's a real struggle, because the author starts off with a thing called a 'radix point'. Hell's bells. Presumably what they mean is the decimal point? Time to look at [[floating-point notation]], where the book completely skips over how to actually interpret the numbers generated by the method. Sigh. Again.
<-- this is where I realised I could have stopped at 1.5 -->
### 1.8 Data compression
### 1.9 Communication errors
