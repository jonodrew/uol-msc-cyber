Source: https://www.cybok.org/media/downloads/Human_Factors_issue_1.0.pdf

>[!abstract] summary
>
>>[!quote] Most [users] choose productivity over security, because that is what the organisation also does
>
>Making security tasks usable means making them fit to the human, and not the other way round. Training and awareness is an example of trying to fit humans to tasks. 
>
>Alarm fatigue is real, and so alarms should be NEAT: necessary, explainable, actionable, and tested. The fact this acronym comes from someone at [[Microsoft]] is a little too ironic. You know, the folks who managed to make this happen:
>
>![[Pasted image 20230514145759.png]]
>
>>[!quote] \[...] people often \[work in insecure ways] because they fear not being able to access \[resources] when they need them
>
>The cost of implementing a [[control]] should include the productivity cost. This might be done through interviews/assessments with experienced staff and managers, or more formally through the Goals, Operators, Methods method
>
>Introduces term 'Compliance Budget' from Beautement et al., to describe the limited amount of patience users have for rules that must be complied with even when they see no value to them
>
>A psychologist called James Reason is my new favorite bit of nominative determinism
>
>>[!quote] A security incident occurs because the threat finds its way through a series of vulnerabilities in the organisationâ€™s defences. A person may be the one who pushed the wrong button or clicked on the link and caused the incident. However, several other failures preceded this, leading to that person being put in a position where making what appeared the right choice turned out to be the wrong one.
>
>Ughh this whole thing is so quotable
>
>>[!quote] Never give an order that can't be obeyed
>>
>> \- General MacArthur
>>
>
>Security policies that are impossible to comply with result in users disregarding the author entirely, reasoning (often correctly) that they do not understand the work and can therefore be safely ignored. This isn't true, of course - security professionals understand how to secure _things_ - but without a deep understanding of how this applies to the context, issuing directives that can't be followed is a great way to make yourself irrelevant and ensure you're viewed with contempt
>
>Awareness, education, and training are three separate steps
>
>Training must include a safe space for experimentation and therefore tolerance for getting things wrong
>
>[[phishing attacks]] against your own people may decrease trust, which we've established as pretty fuckin' important. Instead of doing this heinous and stupid thing, consider [[DMARC]]!
>
>PUT DEVELOPERS ON SECOND LINE ğŸ’…ğŸ» they absolutely should experience the consequences of their work. Fast feedback loops ftw


